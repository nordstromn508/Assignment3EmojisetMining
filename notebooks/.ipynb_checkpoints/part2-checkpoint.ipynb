{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset format\n",
    "An event is a list of emojis.\n",
    "\n",
    "A sequence is a list of events.\n",
    "\n",
    "A dataset is a list of sequences.\n",
    "\n",
    "Thus, a dataset is a list of lists of lists of emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example =  [\n",
    "    [[\"a\"], [\"a\", \"b\", \"c\"], [\"a\", \"c\"], [\"c\"]],\n",
    "    [[\"a\"], [\"c\"], [\"b\", \"c\"]],\n",
    "    [[\"a\", \"b\"], [\"d\"], [\"c\"], [\"b\"], [\"c\"]],\n",
    "    [[\"a\"], [\"c\"], [\"b\"], [\"c\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['💃', '🏼', '🤪', '😎', '🤠', '🌞'], ['🙄', '😤', '😠', '✋', '🌩']],\n",
       " [['💃', '🏼', '🤪', '😎', '🤠', '🌞'], ['🙄', '😤', '😠', '✋', '🌩']],\n",
       " [['🤠'], ['😎']],\n",
       " [['🐷'], ['😎', '🤠', '😊']],\n",
       " [['💃', '🏼', '🤪', '😎', '🤠', '🌞'], ['🙄', '😤', '😠', '✋', '🌩']]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert emoji data set to the above format\n",
    "data = pd.read_excel('../Emojisets_.xlsx')\n",
    "emojiset = data.iloc[1:, 1]\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(emojiset)):\n",
    "    seq = []\n",
    "    emojiset_split = emojiset.iloc[i][:].split(',')\n",
    "    for combined_emojis in emojiset_split:\n",
    "        event = list(combined_emojis)\n",
    "        seq.append(event)\n",
    "    dataset.append(seq)\n",
    "    \n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations\n",
    "### Subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simple recursive method that checks if subsequence is a subSequence of mainSequence\n",
    "\"\"\"\n",
    "def isSubsequence(mainSequence, subSequence):\n",
    "    subSequenceClone = list(subSequence) # clone the sequence, because we will alter it\n",
    "    return isSubsequenceRecursive(mainSequence, subSequenceClone) #start recursion\n",
    "\n",
    "\"\"\"\n",
    "Function for the recursive call of isSubsequence, not intended for external calls\n",
    "\"\"\"\n",
    "def isSubsequenceRecursive(mainSequence, subSequenceClone, start=0):\n",
    "    # Check if empty: End of recursion, all itemsets have been found\n",
    "    if (not subSequenceClone):\n",
    "        return True\n",
    "    # retrieves element of the subsequence and removes is from subsequence \n",
    "    firstElem = set(subSequenceClone.pop(0))\n",
    "    # Search for the first itemset...\n",
    "    for i in range(start, len(mainSequence)):\n",
    "        if (set(mainSequence[i]).issuperset(firstElem)):\n",
    "            # and recurse\n",
    "            return isSubsequenceRecursive(mainSequence, subSequenceClone, i + 1)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['💃', '🏼', '🤪', '😎', '🤠', '🌞'], ['🙄', '😤', '😠', '✋', '🌩']]\n"
     ]
    }
   ],
   "source": [
    "aSequence = dataset[1]\n",
    "print( aSequence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSubsequence(aSequence, [[\"😎\"], [\"🙄\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSubsequence(aSequence, [[\"😎\"], [\"🤠\", \"🌞\"], [\"✋\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSubsequence(aSequence, [[\"🤠\", \"🌞\"], [\"✋\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of an itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes the length of the sequence (sum of the length of the contained itemsets)\n",
    "\"\"\"\n",
    "def sequenceLength(sequence):\n",
    "    return sum(len(i) for i in sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['💃', '🏼', '🤪', '😎', '🤠', '🌞'], ['🙄', '😤', '😠', '✋', '🌩']]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print (dataset[0])\n",
    "print (sequenceLength (dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support of a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes the support of a sequence in a dataset\n",
    "\"\"\"\n",
    "def countSupport (dataset, candidateSequence):\n",
    "    return sum(1 for seq in dataset if isSubsequence(seq, candidateSequence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countSupport(dataset, [['😊']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AprioriAll\n",
    "### 1 . Candidate Generation\n",
    "#### For a single pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates one candidate of length k from two candidates of length (k-1) as used in the AprioriAll algorithm\n",
    "\"\"\"\n",
    "def generateCandidatesForPair(cand1, cand2):\n",
    "    cand1Clone = copy.deepcopy(cand1)\n",
    "    cand2Clone = copy.deepcopy(cand2)\n",
    "    # drop the leftmost item from cand1:\n",
    "    if (len (cand1[0]) == 1):\n",
    "        cand1Clone.pop(0)\n",
    "    else:\n",
    "        cand1Clone[0] = cand1Clone[0][1:]\n",
    "    # drop the rightmost item from cand2:\n",
    "    if (len (cand2[-1]) == 1):\n",
    "        cand2Clone.pop(-1)\n",
    "    else:\n",
    "        cand2Clone[-1] = cand2Clone[-1][:-1]\n",
    "    \n",
    "    # if the result is not the same, then we dont need to join\n",
    "    if not cand1Clone == cand2Clone:\n",
    "        return []\n",
    "    else:\n",
    "        newCandidate = copy.deepcopy(cand1)\n",
    "        if (len (cand2[-1]) == 1):\n",
    "            newCandidate.append(cand2[-1])\n",
    "        else:\n",
    "            newCandidate [-1].extend(cand2[-1][-1])\n",
    "        return newCandidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🌞'], ['🤪']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidateA = [[\"🌞\"]]\n",
    "candidateB = [[\"🤪\"]]\n",
    "generateCandidatesForPair(candidateA, candidateB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🌞'], ['🙄', '😤'], ['🤪'], ['✋']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidateA = [['🌞'], ['🙄', '😤'], ['🤪']]\n",
    "candidateC = [['🙄', '😤'], ['🤪'], ['✋']]\n",
    "generateCandidatesForPair(candidateA, candidateC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidateA = [['🌞'], ['🙄', '😤'], ['🤪']]\n",
    "candidateD = [['🌞'], ['🙄', '😤'], ['🤪']]\n",
    "generateCandidatesForPair(candidateA, candidateD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a set of candidates (of the last level):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates the set of candidates of length k from the set of frequent sequences with length (k-1)\n",
    "\"\"\"\n",
    "def generateCandidates(lastLevelCandidates):\n",
    "    k = sequenceLength(lastLevelCandidates[0]) + 1\n",
    "    if (k == 2):\n",
    "        flatShortCandidates = [item for sublist2 in lastLevelCandidates for sublist1 in sublist2 for item in sublist1]\n",
    "        result = [[[a, b]] for a in flatShortCandidates for b in flatShortCandidates if b > a]\n",
    "        result.extend([[[a], [b]] for a in flatShortCandidates for b in flatShortCandidates])\n",
    "        return result\n",
    "    else:\n",
    "        candidates = []\n",
    "        for i in range(0, len(lastLevelCandidates)):\n",
    "            for j in range(0, len(lastLevelCandidates)):\n",
    "                newCand = generateCandidatesForPair(lastLevelCandidates[i], lastLevelCandidates[j])\n",
    "                if (not newCand == []):\n",
    "                    candidates.append(newCand)\n",
    "        candidates.sort()\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example; Lets assume, we know the frequent sequences of level 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lastLevelFrequentPatterns = [\n",
    "    [['🙄', '😤']], \n",
    "    [['😤', '✋']], \n",
    "    [['🙄'], ['😤']], \n",
    "    [['🙄'], ['✋']], \n",
    "    [['😤'], ['✋']],\n",
    "    [['✋'], ['😤']], \n",
    "    [['✋'], ['✋']],  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the generate candidates for level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['✋'], ['✋'], ['✋']],\n",
       " [['✋'], ['✋'], ['😤']],\n",
       " [['✋'], ['😤'], ['✋']],\n",
       " [['✋'], ['😤', '✋']],\n",
       " [['😤'], ['✋'], ['✋']],\n",
       " [['😤'], ['✋'], ['😤']],\n",
       " [['😤', '✋'], ['✋']],\n",
       " [['😤', '✋'], ['😤']],\n",
       " [['🙄'], ['✋'], ['✋']],\n",
       " [['🙄'], ['✋'], ['😤']],\n",
       " [['🙄'], ['😤'], ['✋']],\n",
       " [['🙄'], ['😤', '✋']],\n",
       " [['🙄', '😤'], ['✋']],\n",
       " [['🙄', '😤', '✋']]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newCandidates = generateCandidates(lastLevelFrequentPatterns)\n",
    "newCandidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . Candidate Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes all direct subsequence for a given sequence.\n",
    "A direct subsequence is any sequence that originates from deleting exactly one item from any event in the original sequence.\n",
    "\"\"\"\n",
    "def generateDirectSubsequences(sequence):\n",
    "    result = []\n",
    "    for i, itemset in enumerate(sequence):\n",
    "        if (len(itemset) == 1):\n",
    "            sequenceClone = copy.deepcopy(sequence)\n",
    "            sequenceClone.pop(i)\n",
    "            result.append(sequenceClone)\n",
    "        else:\n",
    "            for j in range(len(itemset)):\n",
    "                sequenceClone = copy.deepcopy(sequence)\n",
    "                sequenceClone[i].pop(j)\n",
    "                result.append(sequenceClone)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prunes the set of candidates generated for length k given all frequent sequence of level (k-1), as done in AprioriAll\n",
    "\"\"\"\n",
    "def pruneCandidates(candidatesLastLevel, candidatesGenerated):\n",
    "    return [cand for cand in candidatesGenerated if all(x in candidatesLastLevel for x in generateDirectSubsequences(cand))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this on example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['✋'], ['✋'], ['✋']],\n",
       " [['✋'], ['✋'], ['😤']],\n",
       " [['✋'], ['😤'], ['✋']],\n",
       " [['✋'], ['😤', '✋']],\n",
       " [['😤'], ['✋'], ['✋']],\n",
       " [['😤', '✋'], ['✋']],\n",
       " [['🙄'], ['✋'], ['✋']],\n",
       " [['🙄'], ['✋'], ['😤']],\n",
       " [['🙄'], ['😤'], ['✋']],\n",
       " [['🙄'], ['😤', '✋']],\n",
       " [['🙄', '😤'], ['✋']]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidatesPruned = pruneCandidates(lastLevelFrequentPatterns, newCandidates)\n",
    "candidatesPruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count Candidates (and filter not frequent ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['🙄', '😤'], ['✋']], 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minSupport = 1\n",
    "candidatesCounts = [(i, countSupport(dataset, i)) for i in candidatesPruned]\n",
    "resultLvl = [(i, count) for (i, count) in candidatesCounts if (count >= minSupport)]\n",
    "resultLvl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The AprioriAll algorithm. Computes the frequent sequences in a seqeunce dataset for a given minSupport\n",
    "\n",
    "Args:\n",
    "    dataset: A list of sequences, for which the frequent (sub-)sequences are computed\n",
    "    minSupport: The minimum support that makes a sequence frequent\n",
    "    verbose: If true, additional information on the mining process is printed (i.e., candidates on each level)\n",
    "Returns:\n",
    "    A list of tuples (s, c), where s is a frequent sequence, and c is the count for that sequence\n",
    "\"\"\"\n",
    "def apriori(dataset, minSupport, verbose=False):\n",
    "    global numberOfCountingOperations\n",
    "    numberOfCountingOperations = 0\n",
    "    Overall = []\n",
    "    itemsInDataset = sorted(set ([item for sublist1 in dataset for sublist2 in sublist1 for item in sublist2]))\n",
    "    singleItemSequences = [[[item]] for item in itemsInDataset]\n",
    "    singleItemCounts = [(i, countSupport(dataset, i)) for i in singleItemSequences if countSupport(dataset, i) >= minSupport]\n",
    "    Overall.append(singleItemCounts)\n",
    "    print (\"Result, lvl 1: \" + str(Overall[0]))\n",
    "    k = 1\n",
    "    while (True):\n",
    "        if not Overall [k - 1]:\n",
    "            break\n",
    "        # 1. Candidate generation\n",
    "        candidatesLastLevel = [x[0] for x in Overall[k - 1]]\n",
    "        candidatesGenerated = generateCandidates (candidatesLastLevel)\n",
    "        # 2. Candidate pruning (using a \"containsall\" subsequences)\n",
    "        candidatesPruned = [cand for cand in candidatesGenerated if all(x in candidatesLastLevel for x in generateDirectSubsequences(cand))]\n",
    "        # 3. Candidate checking\n",
    "        candidatesCounts = [(i, countSupport(dataset, i)) for i in candidatesPruned]\n",
    "        resultLvl = [(i, count) for (i, count) in candidatesCounts if (count >= minSupport)]\n",
    "        if verbose:\n",
    "            print (\"Candidates generated, lvl \" + str(k + 1) + \": \" + str(candidatesGenerated))\n",
    "            print (\"Candidates pruned, lvl \" + str(k + 1) + \": \" + str(candidatesPruned))\n",
    "            print (\"Result, lvl \" + str(k + 1) + \": \" + str(resultLvl))\n",
    "        Overall.append(resultLvl)\n",
    "        k = k + 1\n",
    "    # \"flatten\" Overall\n",
    "    Overall = Overall [:-1]\n",
    "    Overall = [item for sublist in Overall for item in sublist]\n",
    "    return Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result, lvl 1: [([['️']], 206), ([['🐤']], 120), ([['🐦']], 178), ([['🔥']], 197), ([['😍']], 101), ([['😎']], 1000), ([['😘']], 110), ([['🤗']], 102), ([['🤠']], 1000), ([['🥰']], 233)]\n",
      "Candidates generated, lvl 2: [[['️', '🐤']], [['️', '🐦']], [['️', '🔥']], [['️', '😍']], [['️', '😎']], [['️', '😘']], [['️', '🤗']], [['️', '🤠']], [['️', '🥰']], [['🐤', '🐦']], [['🐤', '🔥']], [['🐤', '😍']], [['🐤', '😎']], [['🐤', '😘']], [['🐤', '🤗']], [['🐤', '🤠']], [['🐤', '🥰']], [['🐦', '🔥']], [['🐦', '😍']], [['🐦', '😎']], [['🐦', '😘']], [['🐦', '🤗']], [['🐦', '🤠']], [['🐦', '🥰']], [['🔥', '😍']], [['🔥', '😎']], [['🔥', '😘']], [['🔥', '🤗']], [['🔥', '🤠']], [['🔥', '🥰']], [['😍', '😎']], [['😍', '😘']], [['😍', '🤗']], [['😍', '🤠']], [['😍', '🥰']], [['😎', '😘']], [['😎', '🤗']], [['😎', '🤠']], [['😎', '🥰']], [['😘', '🤗']], [['😘', '🤠']], [['😘', '🥰']], [['🤗', '🤠']], [['🤗', '🥰']], [['🤠', '🥰']], [['️'], ['️']], [['️'], ['🐤']], [['️'], ['🐦']], [['️'], ['🔥']], [['️'], ['😍']], [['️'], ['😎']], [['️'], ['😘']], [['️'], ['🤗']], [['️'], ['🤠']], [['️'], ['🥰']], [['🐤'], ['️']], [['🐤'], ['🐤']], [['🐤'], ['🐦']], [['🐤'], ['🔥']], [['🐤'], ['😍']], [['🐤'], ['😎']], [['🐤'], ['😘']], [['🐤'], ['🤗']], [['🐤'], ['🤠']], [['🐤'], ['🥰']], [['🐦'], ['️']], [['🐦'], ['🐤']], [['🐦'], ['🐦']], [['🐦'], ['🔥']], [['🐦'], ['😍']], [['🐦'], ['😎']], [['🐦'], ['😘']], [['🐦'], ['🤗']], [['🐦'], ['🤠']], [['🐦'], ['🥰']], [['🔥'], ['️']], [['🔥'], ['🐤']], [['🔥'], ['🐦']], [['🔥'], ['🔥']], [['🔥'], ['😍']], [['🔥'], ['😎']], [['🔥'], ['😘']], [['🔥'], ['🤗']], [['🔥'], ['🤠']], [['🔥'], ['🥰']], [['😍'], ['️']], [['😍'], ['🐤']], [['😍'], ['🐦']], [['😍'], ['🔥']], [['😍'], ['😍']], [['😍'], ['😎']], [['😍'], ['😘']], [['😍'], ['🤗']], [['😍'], ['🤠']], [['😍'], ['🥰']], [['😎'], ['️']], [['😎'], ['🐤']], [['😎'], ['🐦']], [['😎'], ['🔥']], [['😎'], ['😍']], [['😎'], ['😎']], [['😎'], ['😘']], [['😎'], ['🤗']], [['😎'], ['🤠']], [['😎'], ['🥰']], [['😘'], ['️']], [['😘'], ['🐤']], [['😘'], ['🐦']], [['😘'], ['🔥']], [['😘'], ['😍']], [['😘'], ['😎']], [['😘'], ['😘']], [['😘'], ['🤗']], [['😘'], ['🤠']], [['😘'], ['🥰']], [['🤗'], ['️']], [['🤗'], ['🐤']], [['🤗'], ['🐦']], [['🤗'], ['🔥']], [['🤗'], ['😍']], [['🤗'], ['😎']], [['🤗'], ['😘']], [['🤗'], ['🤗']], [['🤗'], ['🤠']], [['🤗'], ['🥰']], [['🤠'], ['️']], [['🤠'], ['🐤']], [['🤠'], ['🐦']], [['🤠'], ['🔥']], [['🤠'], ['😍']], [['🤠'], ['😎']], [['🤠'], ['😘']], [['🤠'], ['🤗']], [['🤠'], ['🤠']], [['🤠'], ['🥰']], [['🥰'], ['️']], [['🥰'], ['🐤']], [['🥰'], ['🐦']], [['🥰'], ['🔥']], [['🥰'], ['😍']], [['🥰'], ['😎']], [['🥰'], ['😘']], [['🥰'], ['🤗']], [['🥰'], ['🤠']], [['🥰'], ['🥰']]]\n",
      "Candidates pruned, lvl 2: [[['️', '🐤']], [['️', '🐦']], [['️', '🔥']], [['️', '😍']], [['️', '😎']], [['️', '😘']], [['️', '🤗']], [['️', '🤠']], [['️', '🥰']], [['🐤', '🐦']], [['🐤', '🔥']], [['🐤', '😍']], [['🐤', '😎']], [['🐤', '😘']], [['🐤', '🤗']], [['🐤', '🤠']], [['🐤', '🥰']], [['🐦', '🔥']], [['🐦', '😍']], [['🐦', '😎']], [['🐦', '😘']], [['🐦', '🤗']], [['🐦', '🤠']], [['🐦', '🥰']], [['🔥', '😍']], [['🔥', '😎']], [['🔥', '😘']], [['🔥', '🤗']], [['🔥', '🤠']], [['🔥', '🥰']], [['😍', '😎']], [['😍', '😘']], [['😍', '🤗']], [['😍', '🤠']], [['😍', '🥰']], [['😎', '😘']], [['😎', '🤗']], [['😎', '🤠']], [['😎', '🥰']], [['😘', '🤗']], [['😘', '🤠']], [['😘', '🥰']], [['🤗', '🤠']], [['🤗', '🥰']], [['🤠', '🥰']], [['️'], ['️']], [['️'], ['🐤']], [['️'], ['🐦']], [['️'], ['🔥']], [['️'], ['😍']], [['️'], ['😎']], [['️'], ['😘']], [['️'], ['🤗']], [['️'], ['🤠']], [['️'], ['🥰']], [['🐤'], ['️']], [['🐤'], ['🐤']], [['🐤'], ['🐦']], [['🐤'], ['🔥']], [['🐤'], ['😍']], [['🐤'], ['😎']], [['🐤'], ['😘']], [['🐤'], ['🤗']], [['🐤'], ['🤠']], [['🐤'], ['🥰']], [['🐦'], ['️']], [['🐦'], ['🐤']], [['🐦'], ['🐦']], [['🐦'], ['🔥']], [['🐦'], ['😍']], [['🐦'], ['😎']], [['🐦'], ['😘']], [['🐦'], ['🤗']], [['🐦'], ['🤠']], [['🐦'], ['🥰']], [['🔥'], ['️']], [['🔥'], ['🐤']], [['🔥'], ['🐦']], [['🔥'], ['🔥']], [['🔥'], ['😍']], [['🔥'], ['😎']], [['🔥'], ['😘']], [['🔥'], ['🤗']], [['🔥'], ['🤠']], [['🔥'], ['🥰']], [['😍'], ['️']], [['😍'], ['🐤']], [['😍'], ['🐦']], [['😍'], ['🔥']], [['😍'], ['😍']], [['😍'], ['😎']], [['😍'], ['😘']], [['😍'], ['🤗']], [['😍'], ['🤠']], [['😍'], ['🥰']], [['😎'], ['️']], [['😎'], ['🐤']], [['😎'], ['🐦']], [['😎'], ['🔥']], [['😎'], ['😍']], [['😎'], ['😎']], [['😎'], ['😘']], [['😎'], ['🤗']], [['😎'], ['🤠']], [['😎'], ['🥰']], [['😘'], ['️']], [['😘'], ['🐤']], [['😘'], ['🐦']], [['😘'], ['🔥']], [['😘'], ['😍']], [['😘'], ['😎']], [['😘'], ['😘']], [['😘'], ['🤗']], [['😘'], ['🤠']], [['😘'], ['🥰']], [['🤗'], ['️']], [['🤗'], ['🐤']], [['🤗'], ['🐦']], [['🤗'], ['🔥']], [['🤗'], ['😍']], [['🤗'], ['😎']], [['🤗'], ['😘']], [['🤗'], ['🤗']], [['🤗'], ['🤠']], [['🤗'], ['🥰']], [['🤠'], ['️']], [['🤠'], ['🐤']], [['🤠'], ['🐦']], [['🤠'], ['🔥']], [['🤠'], ['😍']], [['🤠'], ['😎']], [['🤠'], ['😘']], [['🤠'], ['🤗']], [['🤠'], ['🤠']], [['🤠'], ['🥰']], [['🥰'], ['️']], [['🥰'], ['🐤']], [['🥰'], ['🐦']], [['🥰'], ['🔥']], [['🥰'], ['😍']], [['🥰'], ['😎']], [['🥰'], ['😘']], [['🥰'], ['🤗']], [['🥰'], ['🤠']], [['🥰'], ['🥰']]]\n",
      "Result, lvl 2: [([['️', '😎']], 128), ([['🐤', '🐦']], 108), ([['🐤', '😎']], 120), ([['🐤', '🤠']], 120), ([['🐤', '🥰']], 106), ([['🐦', '🔥']], 131), ([['🐦', '😎']], 178), ([['🐦', '🤠']], 178), ([['🐦', '🥰']], 164), ([['🔥', '😎']], 163), ([['🔥', '🤠']], 187), ([['🔥', '🥰']], 135), ([['😎', '🤠']], 645), ([['😎', '🥰']], 211), ([['🤠', '🥰']], 213), ([['😎'], ['🤠']], 168), ([['🤠'], ['😎']], 230)]\n",
      "Candidates generated, lvl 3: [[['️', '😎'], ['🤠']], [['️', '😎', '🤠']], [['️', '😎', '🥰']], [['🐤', '🐦', '🔥']], [['🐤', '🐦', '😎']], [['🐤', '🐦', '🤠']], [['🐤', '🐦', '🥰']], [['🐤', '😎'], ['🤠']], [['🐤', '😎', '🤠']], [['🐤', '😎', '🥰']], [['🐤', '🤠'], ['😎']], [['🐤', '🤠', '🥰']], [['🐦', '🔥', '😎']], [['🐦', '🔥', '🤠']], [['🐦', '🔥', '🥰']], [['🐦', '😎'], ['🤠']], [['🐦', '😎', '🤠']], [['🐦', '😎', '🥰']], [['🐦', '🤠'], ['😎']], [['🐦', '🤠', '🥰']], [['🔥', '😎'], ['🤠']], [['🔥', '😎', '🤠']], [['🔥', '😎', '🥰']], [['🔥', '🤠'], ['😎']], [['🔥', '🤠', '🥰']], [['😎'], ['🤠'], ['😎']], [['😎'], ['🤠', '🥰']], [['😎', '🤠'], ['😎']], [['😎', '🤠', '🥰']], [['🤠'], ['😎'], ['🤠']], [['🤠'], ['😎', '🤠']], [['🤠'], ['😎', '🥰']]]\n",
      "Candidates pruned, lvl 3: [[['🐤', '🐦', '😎']], [['🐤', '🐦', '🤠']], [['🐤', '🐦', '🥰']], [['🐤', '😎', '🤠']], [['🐤', '😎', '🥰']], [['🐤', '🤠', '🥰']], [['🐦', '🔥', '😎']], [['🐦', '🔥', '🤠']], [['🐦', '🔥', '🥰']], [['🐦', '😎', '🤠']], [['🐦', '😎', '🥰']], [['🐦', '🤠', '🥰']], [['🔥', '😎', '🤠']], [['🔥', '😎', '🥰']], [['🔥', '🤠', '🥰']], [['😎', '🤠', '🥰']]]\n",
      "Result, lvl 3: [([['🐤', '🐦', '😎']], 108), ([['🐤', '🐦', '🤠']], 108), ([['🐤', '😎', '🤠']], 120), ([['🐤', '😎', '🥰']], 106), ([['🐤', '🤠', '🥰']], 106), ([['🐦', '🔥', '😎']], 131), ([['🐦', '🔥', '🤠']], 131), ([['🐦', '🔥', '🥰']], 117), ([['🐦', '😎', '🤠']], 178), ([['🐦', '😎', '🥰']], 164), ([['🐦', '🤠', '🥰']], 164), ([['🔥', '😎', '🤠']], 161), ([['🔥', '😎', '🥰']], 135), ([['🔥', '🤠', '🥰']], 135), ([['😎', '🤠', '🥰']], 209)]\n",
      "Candidates generated, lvl 4: [[['🐤', '🐦', '😎', '🤠']], [['🐤', '🐦', '😎', '🥰']], [['🐤', '🐦', '🤠', '🥰']], [['🐤', '😎', '🤠', '🥰']], [['🐦', '🔥', '😎', '🤠']], [['🐦', '🔥', '😎', '🥰']], [['🐦', '🔥', '🤠', '🥰']], [['🐦', '😎', '🤠', '🥰']], [['🔥', '😎', '🤠', '🥰']]]\n",
      "Candidates pruned, lvl 4: [[['🐤', '🐦', '😎', '🤠']], [['🐤', '😎', '🤠', '🥰']], [['🐦', '🔥', '😎', '🤠']], [['🐦', '🔥', '😎', '🥰']], [['🐦', '🔥', '🤠', '🥰']], [['🐦', '😎', '🤠', '🥰']], [['🔥', '😎', '🤠', '🥰']]]\n",
      "Result, lvl 4: [([['🐤', '🐦', '😎', '🤠']], 108), ([['🐤', '😎', '🤠', '🥰']], 106), ([['🐦', '🔥', '😎', '🤠']], 131), ([['🐦', '🔥', '😎', '🥰']], 117), ([['🐦', '🔥', '🤠', '🥰']], 117), ([['🐦', '😎', '🤠', '🥰']], 164), ([['🔥', '😎', '🤠', '🥰']], 135)]\n",
      "Candidates generated, lvl 5: [[['🐤', '🐦', '😎', '🤠', '🥰']], [['🐦', '🔥', '😎', '🤠', '🥰']]]\n",
      "Candidates pruned, lvl 5: [[['🐦', '🔥', '😎', '🤠', '🥰']]]\n",
      "Result, lvl 5: [([['🐦', '🔥', '😎', '🤠', '🥰']], 117)]\n",
      "Candidates generated, lvl 6: []\n",
      "Candidates pruned, lvl 6: []\n",
      "Result, lvl 6: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([['️']], 206),\n",
       " ([['🐤']], 120),\n",
       " ([['🐦']], 178),\n",
       " ([['🔥']], 197),\n",
       " ([['😍']], 101),\n",
       " ([['😎']], 1000),\n",
       " ([['😘']], 110),\n",
       " ([['🤗']], 102),\n",
       " ([['🤠']], 1000),\n",
       " ([['🥰']], 233),\n",
       " ([['️', '😎']], 128),\n",
       " ([['🐤', '🐦']], 108),\n",
       " ([['🐤', '😎']], 120),\n",
       " ([['🐤', '🤠']], 120),\n",
       " ([['🐤', '🥰']], 106),\n",
       " ([['🐦', '🔥']], 131),\n",
       " ([['🐦', '😎']], 178),\n",
       " ([['🐦', '🤠']], 178),\n",
       " ([['🐦', '🥰']], 164),\n",
       " ([['🔥', '😎']], 163),\n",
       " ([['🔥', '🤠']], 187),\n",
       " ([['🔥', '🥰']], 135),\n",
       " ([['😎', '🤠']], 645),\n",
       " ([['😎', '🥰']], 211),\n",
       " ([['🤠', '🥰']], 213),\n",
       " ([['😎'], ['🤠']], 168),\n",
       " ([['🤠'], ['😎']], 230),\n",
       " ([['🐤', '🐦', '😎']], 108),\n",
       " ([['🐤', '🐦', '🤠']], 108),\n",
       " ([['🐤', '😎', '🤠']], 120),\n",
       " ([['🐤', '😎', '🥰']], 106),\n",
       " ([['🐤', '🤠', '🥰']], 106),\n",
       " ([['🐦', '🔥', '😎']], 131),\n",
       " ([['🐦', '🔥', '🤠']], 131),\n",
       " ([['🐦', '🔥', '🥰']], 117),\n",
       " ([['🐦', '😎', '🤠']], 178),\n",
       " ([['🐦', '😎', '🥰']], 164),\n",
       " ([['🐦', '🤠', '🥰']], 164),\n",
       " ([['🔥', '😎', '🤠']], 161),\n",
       " ([['🔥', '😎', '🥰']], 135),\n",
       " ([['🔥', '🤠', '🥰']], 135),\n",
       " ([['😎', '🤠', '🥰']], 209),\n",
       " ([['🐤', '🐦', '😎', '🤠']], 108),\n",
       " ([['🐤', '😎', '🤠', '🥰']], 106),\n",
       " ([['🐦', '🔥', '😎', '🤠']], 131),\n",
       " ([['🐦', '🔥', '😎', '🥰']], 117),\n",
       " ([['🐦', '🔥', '🤠', '🥰']], 117),\n",
       " ([['🐦', '😎', '🤠', '🥰']], 164),\n",
       " ([['🔥', '😎', '🤠', '🥰']], 135),\n",
       " ([['🐦', '🔥', '😎', '🤠', '🥰']], 117)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apriori(dataset, 100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrefixSpan\n",
    "\n",
    "### Project a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Projects a sequence according to a given prefix, as done in PrefixSpan\n",
    "\n",
    "Args:\n",
    "    sequence: the sequence the projection is built from\n",
    "    prefix: the prefix that is searched for in the sequence\n",
    "    newEvent: if set to True, the first itemset is ignored\n",
    "Returns:\n",
    "    If the sequence does not contain the prefix, then None.\n",
    "    Otherwise, a new sequence starting from the position of the prefix, including the itemset that includes the prefix\n",
    "\"\"\"\n",
    "def projectSequence(sequence, prefix, newEvent):\n",
    "    result = None\n",
    "    for i, itemset in enumerate(sequence):\n",
    "        if result is None:\n",
    "            if (not newEvent) or i > 0:\n",
    "                if (all(x in itemset for x in prefix)):\n",
    "                    result = [list(itemset)]\n",
    "        else:\n",
    "            result.append(copy.copy(itemset))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['😎', '🤠'], ['🐦', '🤠'], ['🤠']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [[\"🐦\"], [\"😎\", \"🤠\"], [\"🐦\", \"🤠\"], [\"🤠\"]]\n",
    "projectSequence(seq, [\"😎\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🐦', '🤠'], ['🤠']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectSequence(seq, [\"🐦\", \"🤠\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🐦'], ['😎', '🤠'], ['🐦', '🤠'], ['🤠']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectSequence(seq, [\"🐦\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🐦', '🤠'], ['🤠']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectSequence(seq, [\"🐦\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Projects a dataset according to a given prefix, as done in PrefixSpan\n",
    "\n",
    "Args:\n",
    "    dataset: the dataset the projection is built from\n",
    "    prefix: the prefix that is searched for in the sequence\n",
    "    newEvent: if set to True, the first itemset is ignored\n",
    "Returns:\n",
    "    A (potentially empty) list of sequences\n",
    "\"\"\"\n",
    "def projectDatabase(dataset, prefix, newEvent):\n",
    "    projectedDB = []\n",
    "    for sequence in dataset:\n",
    "        seqProjected = projectSequence(sequence, prefix, newEvent)\n",
    "        if not seqProjected is None:\n",
    "            projectedDB.append(seqProjected)\n",
    "    return projectedDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤']],\n",
       " [['😎', '🤠', '🥰', '🐦', '🐤', '🔥']]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectDatabase(dataset, [\"🐦\"], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some more utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates a list of all items that are contained in a dataset\n",
    "\"\"\"\n",
    "def generateItems(dataset):\n",
    "    return sorted(set ([item for sublist1 in dataset for sublist2 in sublist1 for item in sublist2]))\n",
    "\n",
    "\"\"\"\n",
    "Computes a defaultdict that maps each item in the dataset to its support\n",
    "\"\"\"\n",
    "def generateItemSupports(dataset, ignoreFirstEvent=False, prefix=[]):\n",
    "    result = defaultdict(int)\n",
    "    for sequence in dataset:\n",
    "        if ignoreFirstEvent:\n",
    "            sequence = sequence[1:]\n",
    "        cooccurringItems = set()\n",
    "        for itemset in sequence:\n",
    "            if all(x in itemset for x in prefix):\n",
    "                for item in itemset:\n",
    "                    if not item in prefix:\n",
    "                        cooccurringItems.add(item)\n",
    "        for item in cooccurringItems:\n",
    "            result [item] += 1\n",
    "    return sorted(result.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The PrefixSpan algorithm. Computes the frequent sequences in a seqeunce dataset for a given minSupport\n",
    "\n",
    "Args:\n",
    "    dataset: A list of sequences, for which the frequent (sub-)sequences are computed\n",
    "    minSupport: The minimum support that makes a sequence frequent\n",
    "Returns:\n",
    "    A list of tuples (s, c), where s is a frequent sequence, and c is the count for that sequence\n",
    "\"\"\"\n",
    "def prefixSpan(dataset, minSupport):\n",
    "    result = []\n",
    "    itemCounts = generateItemSupports(dataset)\n",
    "    for item, count in itemCounts:\n",
    "        if count >= minSupport:\n",
    "            newPrefix = [[item]]\n",
    "            result.append((newPrefix, count))\n",
    "            result.extend(prefixSpanInternal(projectDatabase(dataset, [item], False), minSupport, newPrefix))\n",
    "    return result\n",
    "\n",
    "def prefixSpanInternal(dataset, minSupport, prevPrefixes=[]):\n",
    "    result = []\n",
    "    \n",
    "    # Add a new item to the last element (==same time)\n",
    "    itemCountSameEvent = generateItemSupports(dataset, False, prefix=prevPrefixes[-1])\n",
    "    for item, count in itemCountSameEvent:\n",
    "        if (count >= minSupport) and item > prevPrefixes[-1][-1]:\n",
    "            newPrefix = copy.deepcopy(prevPrefixes)\n",
    "            newPrefix[-1].append(item)\n",
    "            result.append((newPrefix, count))\n",
    "            result.extend(prefixSpanInternal(projectDatabase(dataset, newPrefix[-1], False), minSupport, newPrefix))\n",
    "        \n",
    "    # Add a new event to the prefix\n",
    "    itemCountSubsequentEvents = generateItemSupports(dataset, True)\n",
    "    for item, count in itemCountSubsequentEvents:\n",
    "        if count >= minSupport:\n",
    "            newPrefix = copy.deepcopy(prevPrefixes)\n",
    "            newPrefix.append([item])\n",
    "            result.append((newPrefix, count))\n",
    "            result.extend(prefixSpanInternal(projectDatabase(dataset, [item], True), minSupport, newPrefix))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['️']], 206),\n",
       " ([['️', '😎']], 128),\n",
       " ([['🐤']], 120),\n",
       " ([['🐤', '🐦']], 108),\n",
       " ([['🐤', '🐦', '😎']], 108),\n",
       " ([['🐤', '🐦', '😎', '🤠']], 108),\n",
       " ([['🐤', '🐦', '🤠']], 108),\n",
       " ([['🐤', '😎']], 120),\n",
       " ([['🐤', '😎', '🤠']], 120),\n",
       " ([['🐤', '😎', '🤠', '🥰']], 106),\n",
       " ([['🐤', '😎', '🥰']], 106),\n",
       " ([['🐤', '🤠']], 120),\n",
       " ([['🐤', '🤠', '🥰']], 106),\n",
       " ([['🐤', '🥰']], 106),\n",
       " ([['🐦']], 178),\n",
       " ([['🐦', '🔥']], 131),\n",
       " ([['🐦', '🔥', '😎']], 131),\n",
       " ([['🐦', '🔥', '😎', '🤠']], 131),\n",
       " ([['🐦', '🔥', '😎', '🤠', '🥰']], 117),\n",
       " ([['🐦', '🔥', '😎', '🥰']], 117),\n",
       " ([['🐦', '🔥', '🤠']], 131),\n",
       " ([['🐦', '🔥', '🤠', '🥰']], 117),\n",
       " ([['🐦', '🔥', '🥰']], 117),\n",
       " ([['🐦', '😎']], 178),\n",
       " ([['🐦', '😎', '🤠']], 178),\n",
       " ([['🐦', '😎', '🤠', '🥰']], 164),\n",
       " ([['🐦', '😎', '🥰']], 164),\n",
       " ([['🐦', '🤠']], 178),\n",
       " ([['🐦', '🤠', '🥰']], 164),\n",
       " ([['🐦', '🥰']], 164),\n",
       " ([['🔥']], 197),\n",
       " ([['🔥', '😎']], 163),\n",
       " ([['🔥', '😎', '🤠']], 161),\n",
       " ([['🔥', '😎', '🤠', '🥰']], 135),\n",
       " ([['🔥', '😎', '🥰']], 135),\n",
       " ([['🔥', '🤠']], 187),\n",
       " ([['🔥', '🤠', '🥰']], 135),\n",
       " ([['🔥', '🥰']], 135),\n",
       " ([['😍']], 101),\n",
       " ([['😎']], 1000),\n",
       " ([['😎', '🤠']], 645),\n",
       " ([['😎', '🤠', '🥰']], 209),\n",
       " ([['😎', '🥰']], 211),\n",
       " ([['😎'], ['🤠']], 168),\n",
       " ([['😘']], 110),\n",
       " ([['🤗']], 102),\n",
       " ([['🤠']], 1000),\n",
       " ([['🤠', '🥰']], 213),\n",
       " ([['🤠'], ['😎']], 230),\n",
       " ([['🥰']], 233)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixSpan(dataset, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for closed and maximal patterns\n",
    "### Closed patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a list of all frequent sequences and their counts, compute the set of closed frequent sequence (as a list)\n",
    "This is only a very simplistic (naive) implementation for demonstration purposes!\n",
    "\"\"\"\n",
    "def filterClosed(result):\n",
    "    for supersequence, countSeq in copy.deepcopy(result):\n",
    "        for subsequence, countSubSeq in copy.deepcopy(result):\n",
    "            if isSubsequence(supersequence, subsequence) and (countSeq == countSubSeq) and subsequence != supersequence:\n",
    "                result.remove((subsequence, countSubSeq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['️']], 206),\n",
       " ([['️', '😎']], 128),\n",
       " ([['🐤', '🐦', '😎', '🤠']], 108),\n",
       " ([['🐤', '😎', '🤠']], 120),\n",
       " ([['🐤', '😎', '🤠', '🥰']], 106),\n",
       " ([['🐦', '🔥', '😎', '🤠']], 131),\n",
       " ([['🐦', '🔥', '😎', '🤠', '🥰']], 117),\n",
       " ([['🐦', '😎', '🤠']], 178),\n",
       " ([['🐦', '😎', '🤠', '🥰']], 164),\n",
       " ([['🔥']], 197),\n",
       " ([['🔥', '😎']], 163),\n",
       " ([['🔥', '😎', '🤠']], 161),\n",
       " ([['🔥', '😎', '🤠', '🥰']], 135),\n",
       " ([['🔥', '🤠']], 187),\n",
       " ([['😍']], 101),\n",
       " ([['😎']], 1000),\n",
       " ([['😎', '🤠']], 645),\n",
       " ([['😎', '🤠', '🥰']], 209),\n",
       " ([['😎', '🥰']], 211),\n",
       " ([['😎'], ['🤠']], 168),\n",
       " ([['😘']], 110),\n",
       " ([['🤗']], 102),\n",
       " ([['🤠']], 1000),\n",
       " ([['🤠', '🥰']], 213),\n",
       " ([['🤠'], ['😎']], 230),\n",
       " ([['🥰']], 233)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prefixSpan(dataset, 100)\n",
    "filterClosed(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a list of all frequent sequences and their counts, compute the set of maximal frequent sequence (as a list)\n",
    "This is only a very naive implementation for demonstration purposes!\n",
    "\"\"\"\n",
    "def filterMaximal(result):\n",
    "    for supersequence, countSeq in copy.deepcopy(result):\n",
    "        for subsequence, countSubSeq in copy.deepcopy(result):\n",
    "            if isSubsequence (supersequence, subsequence) and subsequence != supersequence:\n",
    "                result.remove((subsequence, countSubSeq)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['️', '😎']], 128),\n",
       " ([['🐤', '🐦', '😎', '🤠']], 108),\n",
       " ([['🐤', '😎', '🤠', '🥰']], 106),\n",
       " ([['🐦', '🔥', '😎', '🤠', '🥰']], 117),\n",
       " ([['😍']], 101),\n",
       " ([['😎'], ['🤠']], 168),\n",
       " ([['😘']], 110),\n",
       " ([['🤗']], 102),\n",
       " ([['🤠'], ['😎']], 230)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prefixSpan (dataset, 100)\n",
    "filterMaximal(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
